{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Welcome to FUAM Deployment\n",
    "\n",
    "This notebook deployes the latest FUAM version in the specified workspace. It works for initial deployment and for the upgrade process of FUAM.\n",
    "\n",
    "**End-to-end documenation on fabric-toolbox:**\n",
    "\n",
    "[Visit - How to deploy and configure FUAM](https://github.com/microsoft/fabric-toolbox/blob/main/monitoring/fabric-unified-admin-monitoring/how-to/How_to_deploy_FUAM.md)\n",
    "\n",
    "**What is happening in this notebook?**\n",
    " - The notebook checks the two cloud connections for FUAM (if initial deployment, connections will be created, otherwise check only)\n",
    " - It downloads the latest FUAM src files from Github\n",
    " - It deploys/updates the Fabric items in the current workspace\n",
    " - It creates all needed tables automatically, so reports work also with some data missing\n",
    "\n",
    "**Next steps**\n",
    "- (Optional) Change connection names, only if needed\n",
    "- Run this notebook\n",
    "\n",
    "If you **deploy** FUAM in this workspace at the **first time**:\n",
    "- Navigate to the cloud connections\n",
    "- Search under cloud connection for **fuam fabric-service-api admin** and for **fuam pbi-service-api admin** \n",
    "- Add the credentials of your service principal to these connections\n",
    "\n",
    "If you **update** your existing FUAM workspace:\n",
    "- After the notebooks has been executed, you are **done**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:15:20.7412529Z",
       "execution_start_time": "2025-12-18T10:15:07.5156281Z",
       "normalized_state": "finished",
       "parent_msg_id": "9eb57d59-a13d-459c-a9dc-e8fc35da256a",
       "queued_time": "2025-12-18T10:15:02.4856251Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": "2025-12-18T10:15:02.4864157Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ms-fabric-cli\n",
      "  Downloading ms_fabric_cli-1.3.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Requirement already satisfied: msal<2,>=1.29 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal[broker]<2,>=1.29->ms-fabric-cli) (1.33.0)\n",
      "Requirement already satisfied: msal_extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.3.1)\n",
      "Collecting questionary (from ms-fabric-cli)\n",
      "  Downloading questionary-2.1.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: prompt_toolkit>=3.0.41 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (3.0.52)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (5.5.2)\n",
      "Requirement already satisfied: jmespath in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.0.1)\n",
      "Requirement already satisfied: pyyaml==6.0.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (6.0.2)\n",
      "Collecting argcomplete>=3.6.2 (from ms-fabric-cli)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: psutil==7.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (7.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.32.5)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.8.0)\n",
      "Requirement already satisfied: cryptography<48,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (45.0.6)\n",
      "Collecting pymsalruntime<0.19,>=0.18 (from msal[broker]<2,>=1.29->ms-fabric-cli)\n",
      "  Downloading pymsalruntime-0.18.1-cp311-cp311-manylinux_2_35_x86_64.whl.metadata (264 bytes)\n",
      "Requirement already satisfied: wcwidth in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from prompt_toolkit>=3.0.41->ms-fabric-cli) (0.2.13)\n",
      "Requirement already satisfied: cffi>=1.14 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cryptography<48,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (1.17.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2024.7.4)\n",
      "Requirement already satisfied: pycparser in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cffi>=1.14->cryptography<48,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.22)\n",
      "Downloading ms_fabric_cli-1.3.1-py3-none-any.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.5/319.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading questionary-2.1.1-py3-none-any.whl (36 kB)\n",
      "Downloading pymsalruntime-0.18.1-cp311-cp311-manylinux_2_35_x86_64.whl (93.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymsalruntime, argcomplete, questionary, ms-fabric-cli\n",
      "Successfully installed argcomplete-3.6.3 ms-fabric-cli-1.3.1 pymsalruntime-0.18.1 questionary-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e022b8-0d0d-4236-857c-6d253c78122d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:15:21.0834435Z",
       "execution_start_time": "2025-12-18T10:15:20.7422411Z",
       "normalized_state": "finished",
       "parent_msg_id": "ce77ff01-7d06-46e1-89b1-f7c32ff276d1",
       "queued_time": "2025-12-18T10:15:02.4889779Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbi_connection_name = 'fuam pbi-service-api admin'\n",
    "fabric_connection_name = 'fuam fabric-service-api admin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb77f4b-4cc8-49c6-83cd-231680a1d618",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import of needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:15:29.1826752Z",
       "execution_start_time": "2025-12-18T10:15:21.0846538Z",
       "normalized_state": "finished",
       "parent_msg_id": "11eadf25-e860-4d9a-8542-62c764548e00",
       "queued_time": "2025-12-18T10:15:02.5960237Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download of source & config files\n",
    "This part downloads all source and config files of FUAM needed for the deployment into the ressources of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:36.415405Z",
       "execution_start_time": "2025-12-18T10:15:29.1836919Z",
       "normalized_state": "finished",
       "parent_msg_id": "3b95f075-5563-49b6-b24e-9f7e77add906",
       "queued_time": "2025-12-18T10:15:10.808997Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:]).replace(remove_folder_prefix, \"\")), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "repo_owner = \"Microsoft\"\n",
    "repo_name = \"fabric-toolbox\"\n",
    "branch = \"main\"\n",
    "folder_prefix = \"monitoring/fabric-unified-admin-monitoring\"\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}/\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/config\" , remove_folder_prefix = folder_prefix)\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/data/data.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/data\" , remove_folder_prefix = folder_prefix)\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/data/data.zip\", extract_to= \"./builtin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeef159d-0f52-43a1-a895-5c3b293ffc61",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:36.8352149Z",
       "execution_start_time": "2025-12-18T10:17:36.416623Z",
       "normalized_state": "finished",
       "parent_msg_id": "1d6feb43-f6a2-4453-a5a0-3fcd867f5f97",
       "queued_time": "2025-12-18T10:15:11.632338Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        deployment_order = json.load(file)\n",
    "\n",
    "src_workspace_name = config['workspace']\n",
    "src_pbi_connection = config['connections']['pbi_connection']\n",
    "src_fabric_connection = config['connections']['fabric_connection']\n",
    "\n",
    "semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bba6a51-107d-4c10-9a41-5cb2a9e07c27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:37.1937228Z",
       "execution_start_time": "2025-12-18T10:17:36.8364609Z",
       "normalized_state": "finished",
       "parent_msg_id": "fb0b46d9-a52c-47a1-9a4b-0807f97aa878",
       "queued_time": "2025-12-18T10:15:13.1340799Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "    if (capture_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output\n",
    "\n",
    "def fab_get_id(name):\n",
    "    id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def get_id_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"fuam_id\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def copy_to_tmp(name):\n",
    "    \"\"\"Extract files from zip to memory (handles nested folders at any depth).\"\"\"\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    file_contents = {}  # Store file paths and their content in memory\n",
    "    \n",
    "    with ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            # Skip directory entries (ending with /) but include all files at any nesting level\n",
    "            # This handles: src/name/file.txt, src/name/subfolder/file.txt, src/name/a/b/c/file.txt, etc.\n",
    "            if file.startswith(f'src/{name}/') and not file.endswith('/'):\n",
    "                # Read file content into memory instead of extracting to disk\n",
    "                file_contents[file] = archive.read(file)\n",
    "    \n",
    "    return file_contents\n",
    "\n",
    "\n",
    "def replace_ids_in_memory(file_contents, mapping_table):\n",
    "    \"\"\"Replace IDs in memory-stored files.\"\"\"\n",
    "    updated_contents = {}\n",
    "    \n",
    "    for file_path, content_bytes in file_contents.items():\n",
    "        file_name = os.path.basename(file_path)\n",
    "        \n",
    "        # Decode bytes to string\n",
    "        try:\n",
    "            content = content_bytes.decode('utf-8')\n",
    "        except:\n",
    "            # If decoding fails, keep as binary\n",
    "            updated_contents[file_path] = content_bytes\n",
    "            continue\n",
    "        \n",
    "        if file_name.endswith('.ipynb'):\n",
    "            notebook_json = json.loads(content)\n",
    "            dependencies = notebook_json.get('metadata', {}).get('dependencies', {})\n",
    "            depend = json.dumps(dependencies)\n",
    "            for mapping in mapping_table:  \n",
    "                depend = depend.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "            notebook_json['metadata']['dependencies'] = json.loads(depend)\n",
    "            content = json.dumps(notebook_json)\n",
    "            \n",
    "        elif file_name.endswith(('.py', '.json', '.pbir', '.platform', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "            for mapping in mapping_table:  \n",
    "                content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "        \n",
    "        updated_contents[file_path] = content.encode('utf-8')\n",
    "    \n",
    "    return updated_contents\n",
    "\n",
    "def write_memory_to_temp(file_contents, temp_dir):\n",
    "    \"\"\"Write in-memory files to temporary directory (system temp, not builtin storage).\"\"\"\n",
    "    for file_path, content_bytes in file_contents.items():\n",
    "        full_path = os.path.join(temp_dir, file_path)\n",
    "        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n",
    "        with open(full_path, 'wb') as f:\n",
    "            f.write(content_bytes)\n",
    "    return temp_dir\n",
    "\n",
    "def get_semantic_model_id_from_memory(file_contents, name):\n",
    "    \"\"\"Get semantic model ID from in-memory report definition.\"\"\"\n",
    "    definition_path = f'src/{name}/definition.pbir'\n",
    "    if definition_path in file_contents:\n",
    "        content = json.loads(file_contents[definition_path].decode('utf-8'))\n",
    "        semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "        if semantic_model_id:\n",
    "            return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def get_semantic_model_id(report_folder):\n",
    "    definition_file = os.path.join(report_folder, 'definition.pbir')\n",
    "    if os.path.exists(definition_file):\n",
    "        with open(definition_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "            if semantic_model_id:\n",
    "                return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def update_sm_connection_to_fuam_lakehouse_in_memory(file_contents, name):\n",
    "    \"\"\"Update semantic model connection to FUAM lakehouse in memory.\"\"\"\n",
    "    new_sm_db = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output=True, silently_continue=True)\n",
    "    new_lakehouse_sql_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output=True, silently_continue=True)\n",
    "    \n",
    "    expressions_path = f'src/{name}/definition/expressions.tmdl'\n",
    "    if expressions_path in file_contents:\n",
    "        content = file_contents[expressions_path].decode('utf-8')\n",
    "        match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "        if match:\n",
    "            old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "            content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "            file_contents[expressions_path] = content.encode('utf-8')\n",
    "    return file_contents\n",
    "\n",
    "def update_sm_connection_to_fuam_lakehouse(semantic_model_folder):\n",
    "    new_sm_db= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output = True, silently_continue=True)\n",
    "    new_lakehouse_sql_id= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output = True, silently_continue=True)\n",
    "        \n",
    "    expressions_file = os.path.join(semantic_model_folder, 'definition', 'expressions.tmdl')\n",
    "    if os.path.exists(expressions_file):\n",
    "        with open(expressions_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "            if match:\n",
    "                old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "                content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "                with open(expressions_file, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "def update_report_definition_in_memory(file_contents, name):\n",
    "    \"\"\"Update report definition in memory.\"\"\"\n",
    "    semantic_model_id = get_semantic_model_id_from_memory(file_contents, name)\n",
    "    definition_path = f\"src/{name}/definition.pbir\"\n",
    "    \n",
    "    if definition_path in file_contents:\n",
    "        report_definition = json.loads(file_contents[definition_path].decode('utf-8'))\n",
    "        report_definition[\"datasetReference\"][\"byPath\"] = None\n",
    "        \n",
    "        by_connection_obj = {\n",
    "            \"connectionString\": None,\n",
    "            \"pbiServiceModelId\": None,\n",
    "            \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "            \"pbiModelDatabaseName\": semantic_model_id,\n",
    "            \"name\": \"EntityDataSource\",\n",
    "            \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "        }\n",
    "        \n",
    "        report_definition[\"datasetReference\"][\"byConnection\"] = by_connection_obj\n",
    "        file_contents[definition_path] = json.dumps(report_definition, indent=4).encode('utf-8')\n",
    "    \n",
    "    return file_contents\n",
    "\n",
    "def update_report_definition( path): \n",
    "    semantic_model_id = get_semantic_model_id(path)\n",
    "    definition_path = os.path.join(path, \"definition.pbir\")\n",
    "   \n",
    "    with open(definition_path, \"r\", encoding=\"utf8\") as file:\n",
    "        report_definition = json.load(file)\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byPath\"] = None\n",
    "\n",
    "    by_connection_obj = {\n",
    "            \"connectionString\": None,\n",
    "            \"pbiServiceModelId\": None,\n",
    "            \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "            \"pbiModelDatabaseName\": semantic_model_id,\n",
    "            \"name\": \"EntityDataSource\",\n",
    "            \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "        }\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byConnection\"] = by_connection_obj\n",
    "\n",
    "    with open(definition_path, \"w\") as file:\n",
    "            json.dump(report_definition, file, indent=4)\n",
    "\n",
    "def print_color(text, state):\n",
    "    red  = '\\033[91m'\n",
    "    yellow = '\\033[93m'  \n",
    "    green = '\\033[92m'   \n",
    "    white = '\\033[0m'  \n",
    "    if state == \"error\":\n",
    "        print(red, text, white)\n",
    "    elif state == \"warning\":\n",
    "        print(yellow, text, white)\n",
    "    elif state == \"success\":\n",
    "        print(green, text, white)\n",
    "    else:\n",
    "        print(\"\", text)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a24f9d-acfc-41a4-ae24-772e5200f74a",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Creation of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9328d01d-a5ef-4828-b2cf-dcb0d7482e53",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:44.5723305Z",
       "execution_start_time": "2025-12-18T10:17:37.1948885Z",
       "normalized_state": "finished",
       "parent_msg_id": "8c3ed1b7-77c2-4d4f-8ce7-b8fef6564b48",
       "queued_time": "2025-12-18T10:15:15.6416021Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x create: [AlreadyExists] An element with the same name exists\n",
      "\u001b[93m Connection already exists \u001b[0m\n",
      "Connection ID:09286cef-5a4d-4ad3-bf59-b31f070d72c3\n",
      "x create: [AlreadyExists] An element with the same name exists\n",
      "\u001b[93m Connection already exists \u001b[0m\n",
      "Connection ID:644f217a-8761-4f19-b1b7-a40bfe0ec2af\n"
     ]
    }
   ],
   "source": [
    "def create_or_get_connection(name, baseUrl, audience):\n",
    "    try:\n",
    "        run_fab_command(f\"\"\"create .connections/{name}.connection \n",
    "            -P connectionDetails.type=WebForPipeline,connectionDetails.creationMethod=WebForPipeline.Contents,connectionDetails.parameters.baseUrl={baseUrl},connectionDetails.parameters.audience={audience},credentialDetails.type=Anonymous\"\"\")\n",
    "        print_color(\"New connection created. Enter service principal credentials\", \"success\")\n",
    "    except Exception as ex:\n",
    "        print_color(\"Connection already exists\", \"warning\")\n",
    "\n",
    "    conn_id = run_fab_command(f\"get .connections/{name}.Connection -q id\", silently_continue= True, capture_output= True)\n",
    "    print(\"Connection ID:\" + conn_id)\n",
    "    \n",
    "    \n",
    "    return(conn_id)\n",
    "    \n",
    "conn_pbi_service_api_admin = create_or_get_connection(pbi_connection_name, \"https://api.powerbi.com/v1.0/myorg/admin\", \"https://analysis.windows.net/powerbi/api\" )\n",
    "conn_fabric_service_api_admin = create_or_get_connection(fabric_connection_name, \"https://api.fabric.microsoft.com/v1/admin\", \"\thttps://api.fabric.microsoft.com\" )\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_pbi_connection), \"new_id\": conn_pbi_service_api_admin })\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_fabric_connection), \"new_id\": conn_fabric_service_api_admin })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473b6db-df34-4a72-bd1a-05d5ddefa78e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get current Workspace\n",
    "This cell gets the current workspace to deploy FUAM automatically inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d54542c-1e34-434b-bada-d73fbe7c2535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:45.9109632Z",
       "execution_start_time": "2025-12-18T10:17:44.5735614Z",
       "normalized_state": "finished",
       "parent_msg_id": "132eb4ff-40d8-45e0-a44b-cd41e75e40a4",
       "queued_time": "2025-12-18T10:15:17.2597244Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current workspace: FUAM_V202512_Test3\n",
      "Current workspace ID: abdb87ad-37f9-4139-b5ff-a984fc438dfe\n"
     ]
    }
   ],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True, silently_continue=True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")\n",
    "\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_workspace_name + \".Workspace\"), \"new_id\": trg_workspace_id })\n",
    "mapping_table.append({ \"old_id\": \"00000000-0000-0000-0000-000000000000\", \"new_id\": trg_workspace_id })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "107b5e53-bb86-4305-b3f2-62d259d8ad67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:17:46.30401Z",
       "execution_start_time": "2025-12-18T10:17:45.9121259Z",
       "normalized_state": "finished",
       "parent_msg_id": "c3495949-0fe1-43d7-a26c-894509503761",
       "queued_time": "2025-12-18T10:15:18.1464046Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'old_id': '09f68371-365e-3501-a70d-6291901f4ba5',\n",
       "  'new_id': '09286cef-5a4d-4ad3-bf59-b31f070d72c3'},\n",
       " {'old_id': 'e665127a-bc6f-3487-b0ce-d3b2141df298',\n",
       "  'new_id': '644f217a-8761-4f19-b1b7-a40bfe0ec2af'},\n",
       " {'old_id': '88c8d9fa-2c24-3fad-8f46-b36431c7ba1d',\n",
       "  'new_id': 'abdb87ad-37f9-4139-b5ff-a984fc438dfe'},\n",
       " {'old_id': '00000000-0000-0000-0000-000000000000',\n",
       "  'new_id': 'abdb87ad-37f9-4139-b5ff-a984fc438dfe'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315ef8af-ff5a-4323-869c-7198e910c0bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T07:51:22.2509572Z",
       "execution_start_time": "2025-12-18T07:29:53.3360973Z",
       "normalized_state": "finished",
       "parent_msg_id": "4ad4baa8-6d80-4ab3-aece-0884127c9037",
       "queued_time": "2025-12-18T07:27:08.1480404Z",
       "session_id": "d56f00a4-abf1-4583-b343-e9a2cb5e7c5b",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#############################################\n",
      "Deploying FUAM_Lakehouse.Lakehouse\n",
      "* 'FUAM_Lakehouse.Lakehouse' created\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Staging_Lakehouse.Lakehouse\n",
      "* 'FUAM_Staging_Lakehouse.Lakehouse' created\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Config_Lakehouse.Lakehouse\n",
      "* 'FUAM_Config_Lakehouse.Lakehouse' created\n",
      "* 'Generate_Calendar_Table.Notebook' imported\n",
      "* 'Init_FUAM_Lakehouse_Tables.Notebook' imported\n",
      "* 'Load_Items_E2E.DataPipeline' imported\n",
      "* '02_Transfer_Activities_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying 03_Aggregate_Activities_Unit.Notebook\n",
      "* '03_Aggregate_Activities_Unit.Notebook' imported\n",
      "* 'Load_Activities_E2E.DataPipeline' imported\n",
      "* '01_Transfer_Capacities_Unit.Notebook' imported\n",
      "* 'Load_Capacities_E2E.DataPipeline' imported\n",
      "* '01_Transfer_Capacity_Refreshables_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying Load_Capacity_Refreshables_E2E.DataPipeline\n",
      "* 'Load_Capacity_Refreshables_E2E.DataPipeline' imported\n",
      "* '02_Transfer_Workspaces_Unit.Notebook' imported\n",
      "* 'Load_PBI_Workspaces_E2E.DataPipeline' imported\n",
      "* '01_Transfer_Incremental_Inventory_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying Load_Inventory_E2E.DataPipeline\n",
      "* '01_Transfer_Tenant_Admin_Settings_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying Load_Tenant_Settings_E2E.DataPipeline\n",
      "* 'Load_Git_Connections_E2E.DataPipeline' imported\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Backup_Lakehouse.Lakehouse\n",
      "* 'FUAM_Backup_Lakehouse.Lakehouse' created\n",
      "* '02_FUAM_Lakehouse_Optimization.Notebook' imported\n",
      "* 'Maintenance_for_FUAM.DataPipeline' imported\n",
      "* '01_Create_Snapshot_Tables_Unit.Notebook' imported\n",
      "* 'Load_Domains_E2E.DataPipeline' imported\n",
      "\n",
      "#############################################\n",
      "Deploying 01_Transfer_WidelyShared_OrganizationLinks_Unit.Notebook\n",
      "* '01_Transfer_WidelyShared_OrganizationLinks_Unit.Notebook' imported\n",
      "* '01_Transfer_WidelyShared_PublishedToWeb_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying Load_WidelyShared_OrganizationLinks_E2E.DataPipeline\n",
      "* 'Load_WidelyShared_PublishedToWeb_E2E.DataPipeline' imported\n",
      "\n",
      "#############################################\n",
      "Deploying 01_Transfer_CapacityMetricData_Timepoints_Unit.Notebook\n",
      "* '01_Transfer_CapacityMetricData_Timepoints_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying 02_Transfer_CapacityMetricData_ItemKind_Unit.Notebook\n",
      "* '02_Transfer_CapacityMetricData_ItemKind_Unit.Notebook' imported\n",
      "* '03_Transfer_CapacityMetricData_ItemOperation_Unit.Notebook' imported\n",
      "\n",
      "#############################################\n",
      "Deploying Load_Capacity_Metrics_E2E.DataPipeline\n",
      "* 'Load_Capacity_Metrics_E2E.DataPipeline' imported\n",
      "* 'Load_FUAM_Data_E2E.DataPipeline' imported\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Item_SM.SemanticModel\n",
      "* 'FUAM_Item_SM.SemanticModel' imported\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Item_Analyzer_Report.Report\n",
      "x import: [LongRunningOperationFailed] The operation failed: {'errorCode': 'Workload_FailedToParseFile', 'message': \"Cannot read 'definition.pbir'. 'definition.pbir':\\r\\nInvalid type. Expected String but got Null. Path 'datasetReference.byConnection.connectionString', line 6, position 36.\\r\\nProperty 'pbiServiceModelId' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiServiceModelId', line 7, position 32.\\r\\nProperty 'pbiModelVirtualServerName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelVirtualServerName', line 8, position 40.\\r\\nProperty 'pbiModelDatabaseName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelDatabaseName', line 9, position 35.\\r\\nProperty 'name' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.name', line 10, position 19.\\r\\nProperty 'connectionType' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.connectionType', line 11, position 29.\\r\\n\"}\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Core_SM.SemanticModel\n",
      "* 'FUAM_Core_SM.SemanticModel' imported\n",
      "* 'FUAM_Semantic_Model_Meta_Data_Analyzer_SM.SemanticModel' imported\n",
      "x import: [LongRunningOperationFailed] The operation failed: {'errorCode': 'Workload_FailedToParseFile', 'message': \"Cannot read 'definition.pbir'. 'definition.pbir':\\r\\nInvalid type. Expected String but got Null. Path 'datasetReference.byConnection.connectionString', line 6, position 36.\\r\\nProperty 'pbiServiceModelId' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiServiceModelId', line 7, position 32.\\r\\nProperty 'pbiModelVirtualServerName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelVirtualServerName', line 8, position 40.\\r\\nProperty 'pbiModelDatabaseName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelDatabaseName', line 9, position 35.\\r\\nProperty 'name' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.name', line 10, position 19.\\r\\nProperty 'connectionType' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.connectionType', line 11, position 29.\\r\\n\"}\n",
      "\n",
      "#############################################\n",
      "Deploying FUAM_Gateway_Monitoring_From_Files_SM.SemanticModel\n",
      "* 'FUAM_Gateway_Monitoring_From_Files_SM.SemanticModel' imported\n",
      "x import: [LongRunningOperationFailed] The operation failed: {'errorCode': 'Workload_FailedToParseFile', 'message': \"Cannot read 'definition.pbir'. 'definition.pbir':\\r\\nInvalid type. Expected String but got Null. Path 'datasetReference.byConnection.connectionString', line 6, position 36.\\r\\nProperty 'pbiServiceModelId' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiServiceModelId', line 7, position 32.\\r\\nProperty 'pbiModelVirtualServerName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelVirtualServerName', line 8, position 40.\\r\\nProperty 'pbiModelDatabaseName' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.pbiModelDatabaseName', line 9, position 35.\\r\\nProperty 'name' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.name', line 10, position 19.\\r\\nProperty 'connectionType' has not been defined and the schema does not allow additional properties. Path 'datasetReference.byConnection.connectionType', line 11, position 29.\\r\\n\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating a new Lakehouse...\n",
      "Creating a new Lakehouse...\n",
      "Creating a new Lakehouse...\n",
      "Importing '/tmp/tmpyi83ewm6/src/Generate_Calendar_Table.Notebook' → '/FUAM_V202512_Test3.Workspace/Generate_Calendar_Table.Notebook'...\n",
      "Importing '/tmp/tmp97um4esc/src/Init_FUAM_Lakehouse_Tables.Notebook' → '/FUAM_V202512_Test3.Workspace/Init_FUAM_Lakehouse_Tables.Notebook'...\n",
      "Importing '/tmp/tmpxsd2sfyh/src/Check_FUAM_Version.Notebook' → '/FUAM_V202512_Test3.Workspace/Check_FUAM_Version.Notebook'...\n",
      "Importing '/tmp/tmpm0zavrhq/src/01_Transfer_Active_Items_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Active_Items_Unit.Notebook'...\n",
      "Importing '/tmp/tmpupws9rmh/src/Load_Items_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Items_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpdxa41gt3/src/02_Transfer_Activities_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/02_Transfer_Activities_Unit.Notebook'...\n",
      "Importing '/tmp/tmprzxt99uv/src/03_Aggregate_Activities_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/03_Aggregate_Activities_Unit.Notebook'...\n",
      "Importing '/tmp/tmp3z2ly91k/src/Load_Activities_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Activities_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpd1c13uus/src/01_Transfer_Capacities_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Capacities_Unit.Notebook'...\n",
      "Importing '/tmp/tmpvoxaa90t/src/Load_Capacities_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Capacities_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpd3odhy3d/src/01_Transfer_Capacity_Refreshables_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Capacity_Refreshables_Unit.Notebook'...\n",
      "Importing '/tmp/tmp3mvnm2o0/src/Load_Capacity_Refreshables_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Capacity_Refreshables_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmp_xyigae4/src/02_Transfer_Workspaces_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/02_Transfer_Workspaces_Unit.Notebook'...\n",
      "Importing '/tmp/tmp22bzdnrd/src/Load_PBI_Workspaces_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_PBI_Workspaces_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmplxg674kd/src/01_Transfer_Incremental_Inventory_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Incremental_Inventory_Unit.Notebook'...\n",
      "Importing '/tmp/tmpcvy_ntul/src/Load_Inventory_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Inventory_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpa0hq1td3/src/01_Transfer_Delegated_Tenant_Settings_Overrides_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Delegated_Tenant_Settings_Overrides_Unit.Notebook'...\n",
      "Importing '/tmp/tmpvaev2tg7/src/01_Transfer_Tenant_Admin_Settings_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Tenant_Admin_Settings_Unit.Notebook'...\n",
      "Importing '/tmp/tmpy9rjdm1q/src/Load_Tenant_Settings_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Tenant_Settings_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmplw59yx1_/src/Load_Delegated_Tenant_Settings_Overrides_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Delegated_Tenant_Settings_Overrides_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmp7p_hy0vp/src/01_Transfer_Git_Connections_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Git_Connections_Unit.Notebook'...\n",
      "Importing '/tmp/tmpe4soslo1/src/Load_Git_Connections_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Git_Connections_E2E.DataPipeline'...\n",
      "Creating a new Lakehouse...\n",
      "Importing '/tmp/tmp8m5ovlaz/src/01_FUAM_Lakehouse_Backup.Notebook' → '/FUAM_V202512_Test3.Workspace/01_FUAM_Lakehouse_Backup.Notebook'...\n",
      "Importing '/tmp/tmpohz8_gvg/src/02_FUAM_Lakehouse_Optimization.Notebook' → '/FUAM_V202512_Test3.Workspace/02_FUAM_Lakehouse_Optimization.Notebook'...\n",
      "Importing '/tmp/tmp1b9treux/src/Maintenance_for_FUAM.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Maintenance_for_FUAM.DataPipeline'...\n",
      "Importing '/tmp/tmp1mvz938d/src/01_Create_Snapshot_Tables_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Create_Snapshot_Tables_Unit.Notebook'...\n",
      "Importing '/tmp/tmpw3jsisnb/src/01_Transfer_Domains_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_Domains_Unit.Notebook'...\n",
      "Importing '/tmp/tmpg_cyz7jk/src/Load_Domains_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Domains_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmp5rx33wkq/src/01_Transfer_WidelyShared_OrganizationLinks_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_WidelyShared_OrganizationLinks_Unit.Notebook'...\n",
      "Importing '/tmp/tmpbg3vki04/src/01_Transfer_WidelyShared_PublishedToWeb_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_WidelyShared_PublishedToWeb_Unit.Notebook'...\n",
      "Importing '/tmp/tmptkbyl5av/src/Load_WidelyShared_OrganizationLinks_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_WidelyShared_OrganizationLinks_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmp7fd91erx/src/Load_WidelyShared_PublishedToWeb_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_WidelyShared_PublishedToWeb_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpypf7cf7b/src/01_Transfer_CapacityMetricData_Timepoints_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/01_Transfer_CapacityMetricData_Timepoints_Unit.Notebook'...\n",
      "Importing '/tmp/tmpy26wgm3q/src/02_Transfer_CapacityMetricData_ItemKind_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/02_Transfer_CapacityMetricData_ItemKind_Unit.Notebook'...\n",
      "Importing '/tmp/tmpdwfyby9c/src/03_Transfer_CapacityMetricData_ItemOperation_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/03_Transfer_CapacityMetricData_ItemOperation_Unit.Notebook'...\n",
      "Importing '/tmp/tmpi7b15vjc/src/Load_Capacity_Metrics_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_Capacity_Metrics_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpief_rran/src/00_Run_Optimization_Module_for_SM_Unit.Notebook' → '/FUAM_V202512_Test3.Workspace/00_Run_Optimization_Module_for_SM_Unit.Notebook'...\n",
      "Importing '/tmp/tmpqvr5ggt9/src/Load_FUAM_Data_E2E.DataPipeline' → '/FUAM_V202512_Test3.Workspace/Load_FUAM_Data_E2E.DataPipeline'...\n",
      "Importing '/tmp/tmpt4hc45k1/src/FUAM_Item_SM.SemanticModel' → '/FUAM_V202512_Test3.Workspace/FUAM_Item_SM.SemanticModel'...\n",
      "Importing '/tmp/tmp9ykcl3eb/src/FUAM_Item_Analyzer_Report.Report' → '/FUAM_V202512_Test3.Workspace/FUAM_Item_Analyzer_Report.Report'...\n",
      "Importing '/tmp/tmp47d_w1f7/src/FUAM_Core_SM.SemanticModel' → '/FUAM_V202512_Test3.Workspace/FUAM_Core_SM.SemanticModel'...\n",
      "Importing '/tmp/tmpki5jpgw4/src/FUAM_Core_Report.Report' → '/FUAM_V202512_Test3.Workspace/FUAM_Core_Report.Report'...\n",
      "Importing '/tmp/tmpl4736wu4/src/FUAM_Semantic_Model_Meta_Data_Analyzer_SM.SemanticModel' → '/FUAM_V202512_Test3.Workspace/FUAM_Semantic_Model_Meta_Data_Analyzer_SM.SemanticModel'...\n",
      "Importing '/tmp/tmppqosx_db/src/FUAM_Semantic_Model_Meta_Data_Analyzer_Report.Report' → '/FUAM_V202512_Test3.Workspace/FUAM_Semantic_Model_Meta_Data_Analyzer_Report.Report'...\n",
      "Importing '/tmp/tmpb24x463f/src/FUAM_SQL_Endpoint_Analyzer_SM.SemanticModel' → '/FUAM_V202512_Test3.Workspace/FUAM_SQL_Endpoint_Analyzer_SM.SemanticModel'...\n",
      "Importing '/tmp/tmpnf425lpw/src/FUAM_SQL_Endpoint_Analyzer_Report.Report' → '/FUAM_V202512_Test3.Workspace/FUAM_SQL_Endpoint_Analyzer_Report.Report'...\n",
      "Importing '/tmp/tmpu_lke0l3/src/FUAM_Gateway_Monitoring_From_Files_SM.SemanticModel' → '/FUAM_V202512_Test3.Workspace/FUAM_Gateway_Monitoring_From_Files_SM.SemanticModel'...\n",
      "Importing '/tmp/tmpw8nbgjxb/src/FUAM_Gateway_Monitoring_From_Files_Report.Report' → '/FUAM_V202512_Test3.Workspace/FUAM_Gateway_Monitoring_From_Files_Report.Report'...\n"
     ]
    }
   ],
   "source": [
    "exclude = [src_workspace_name + \".Workspace\", src_pbi_connection, src_fabric_connection]\n",
    "\n",
    "for it in deployment_order:\n",
    " \n",
    "    new_id = None\n",
    "    \n",
    "    name = it[\"name\"]\n",
    "    \n",
    "    if name in exclude:\n",
    "            continue\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"Deploying {name}\")\n",
    "\n",
    "    # Copy to memory and replace IDs in-memory\n",
    "    file_contents = copy_to_tmp(name)\n",
    "    file_contents = replace_ids_in_memory(file_contents, mapping_table)\n",
    "\n",
    "    cli_parameter = ''\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter = cli_parameter + \" --format .ipynb\"\n",
    "    elif \"Lakehouse\" in name:\n",
    "        run_fab_command(f\"create /{trg_workspace_name}.Workspace/{name}\" , silently_continue=True )\n",
    "        new_id = fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": get_id_by_name(name), \"new_id\": new_id })\n",
    "        \n",
    "        continue\n",
    "    elif \"Report\" in name:\n",
    "        file_contents = update_report_definition_in_memory(file_contents, name)\n",
    "    elif name in semantic_model_connect_to_lakehouse:\n",
    "        file_contents = update_sm_connection_to_fuam_lakehouse_in_memory(file_contents, name)\n",
    "    \n",
    "    # Use system temp directory (often RAM-based) instead of builtin storage\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        write_memory_to_temp(file_contents, temp_dir)\n",
    "        item_path = os.path.join(temp_dir, f\"src/{name}\")\n",
    "        \n",
    "        run_fab_command(f\"import  /{trg_workspace_name}.Workspace/{name} -i {item_path} -f {cli_parameter} \", silently_continue= True)\n",
    "        new_id= fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": it[\"fuam_id\"], \"new_id\": new_id })\n",
    "        # temp_dir automatically cleaned up when context exits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6a7c3",
   "metadata": {},
   "source": [
    "## Move items into folders\n",
    "The items will be moved into the respective folders. Definition is done in the deployment_config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac639c2a-788f-48db-8f72-2300c873dc4a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T10:55:39.3671122Z",
       "execution_start_time": "2025-12-18T10:52:25.7748665Z",
       "normalized_state": "finished",
       "parent_msg_id": "157220c3-5c00-42a4-98ae-dd703292a61d",
       "queued_time": "2025-12-18T10:52:25.7740021Z",
       "session_id": "c7d8cb47-a194-4e17-a63c-85587e3ebb05",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment\n",
      "* true\n",
      "Move items into folder: Deployment\n",
      "Others\n",
      "* true\n",
      "Move items into folder: Others\n",
      "Active Items\n",
      "* true\n",
      "Move items into folder: Active Items\n",
      "Activities\n",
      "* true\n",
      "Move items into folder: Activities\n",
      "Capacities\n",
      "* true\n",
      "Move items into folder: Capacities\n",
      "Capacity Refreshables\n",
      "* true\n",
      "Move items into folder: Capacity Refreshables\n",
      "Workspaces\n",
      "* true\n",
      "Move items into folder: Workspaces\n",
      "Inventory\n",
      "* true\n",
      "Move items into folder: Inventory\n",
      "Tenant Settings\n",
      "* true\n",
      "Move items into folder: Tenant Settings\n",
      "Git Connections\n",
      "* true\n",
      "Move items into folder: Git Connections\n",
      "Maintenance\n",
      "* true\n",
      "Move items into folder: Maintenance\n",
      "Capacity Metrics\n",
      "* true\n",
      "Move items into folder: Capacity Metrics\n",
      "Reporting\n",
      "* true\n",
      "Move items into folder: Reporting\n",
      "Domains\n",
      "* true\n",
      "Move items into folder: Domains\n",
      "WidelyShared\n",
      "* true\n",
      "Move items into folder: WidelyShared\n",
      "Optimization Module\n",
      "* true\n",
      "Move items into folder: Optimization Module\n"
     ]
    }
   ],
   "source": [
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "items_in_ws =  json.loads(run_fab_command(f'api /workspaces/{trg_workspace_id}/items', capture_output= True))['text']['value']\n",
    "\n",
    "\n",
    "def find_existing_item_id(item_name):\n",
    "    for item in items_in_ws:\n",
    "        if item_name == item['displayName'] + '.' + item['type']:\n",
    "            return item['id']\n",
    "\n",
    "\n",
    "for folder in config['folders']:\n",
    "    print(folder['name'])\n",
    "    folder_name = folder['name']\n",
    "\n",
    "    folder_exists = run_fab_command(f'exists /{trg_workspace_name}.Workspace/{folder_name}.Folder', capture_output= True)\n",
    "    print(folder_exists)\n",
    "    if 'false' in folder_exists:\n",
    "        print(f'Create folder {folder_name}')\n",
    "        run_fab_command(f'create /{trg_workspace_name}.Workspace/{folder_name}.Folder')\n",
    "    \n",
    "    folder_id = run_fab_command(f'get {trg_workspace_name}.Workspace/{folder_name}.Folder -q id',  capture_output= True) \n",
    "    print(f'Move items into folder: {folder_name}')  \n",
    "    item_ids = []\n",
    "    for item in folder['items']:\n",
    "        found_it = find_existing_item_id(item)\n",
    "        if found_it is not None:\n",
    "            item_ids.append(found_it)\n",
    "    it = str(item_ids).replace(\"'\", '\"')\n",
    "    res = run_fab_command(f' api -X post workspaces/{trg_workspace_id}/items/bulkmove  -i \\'{{\"targetFolderId\": \"{folder_id}\", \"items\": {it} }}\\' ', capture_output = True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8b0d2-a877-41e8-928a-65855b12bd1f",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Post-Deployment logic\n",
    "In this separate notebook, all needed tables for FUAM are automatically deployed. Addtionally new columns will be added to lakehouse tables in order to be available for the semantic model. This notebook has been deployed from the source code in the step before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60bb1703-82b2-4257-99c9-9b8d8b9e0ab6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:03:58.193817Z",
       "execution_start_time": "2025-12-18T08:03:58.1167893Z",
       "normalized_state": "finished",
       "parent_msg_id": "81a1db4f-882a-4845-915c-d955fe81c454",
       "queued_time": "2025-12-18T07:27:08.1512958Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": "2025-12-18T08:03:53.5965373Z"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f \n",
    "{   \"defaultLakehouse\": { \"name\": \"FUAM_Config_Lakehouse\" } }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22e172d2-c00c-42d1-8c6a-3d993247a79e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:04:12.0898548Z",
       "execution_start_time": "2025-12-18T08:03:58.8029541Z",
       "normalized_state": "finished",
       "parent_msg_id": "90c2d791-cdb4-4798-a516-1d2372b45a36",
       "queued_time": "2025-12-18T07:27:08.1529083Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ms-fabric-cli\r\n",
      "  Downloading ms_fabric_cli-1.3.1-py3-none-any.whl.metadata (9.8 kB)\r\n",
      "Requirement already satisfied: msal<2,>=1.29 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal[broker]<2,>=1.29->ms-fabric-cli) (1.33.0)\r\n",
      "Requirement already satisfied: msal_extensions in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.3.1)\r\n",
      "Collecting questionary (from ms-fabric-cli)\r\n",
      "  Downloading questionary-2.1.1-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Requirement already satisfied: prompt_toolkit>=3.0.41 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (3.0.52)\r\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (5.5.2)\r\n",
      "Requirement already satisfied: jmespath in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (1.0.1)\r\n",
      "Requirement already satisfied: pyyaml==6.0.2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (6.0.2)\r\n",
      "Collecting argcomplete>=3.6.2 (from ms-fabric-cli)\r\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: psutil==7.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from ms-fabric-cli) (7.0.0)\r\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.32.5)\r\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.8.0)\r\n",
      "Requirement already satisfied: cryptography<48,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (45.0.6)\r\n",
      "Collecting pymsalruntime<0.19,>=0.18 (from msal[broker]<2,>=1.29->ms-fabric-cli)\r\n",
      "  Downloading pymsalruntime-0.18.1-cp311-cp311-manylinux_2_35_x86_64.whl.metadata (264 bytes)\r\n",
      "Requirement already satisfied: wcwidth in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from prompt_toolkit>=3.0.41->ms-fabric-cli) (0.2.13)\r\n",
      "Requirement already satisfied: cffi>=1.14 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cryptography<48,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (1.17.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.4.3)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from requests<3,>=2.0.0->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2024.7.4)\r\n",
      "Requirement already satisfied: pycparser in /home/trusted-service-user/jupyter-env/python3.11/lib/python3.11/site-packages (from cffi>=1.14->cryptography<48,>=2.5->msal<2,>=1.29->msal[broker]<2,>=1.29->ms-fabric-cli) (2.22)\r\n",
      "Downloading ms_fabric_cli-1.3.1-py3-none-any.whl (319 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/319.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m317.4/319.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.5/319.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading questionary-2.1.1-py3-none-any.whl (36 kB)\r\n",
      "Downloading pymsalruntime-0.18.1-cp311-cp311-manylinux_2_35_x86_64.whl (93.3 MB)\r\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/93.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/93.3 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/93.3 MB\u001b[0m \u001b[31m137.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/93.3 MB\u001b[0m \u001b[31m133.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/93.3 MB\u001b[0m \u001b[31m127.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/93.3 MB\u001b[0m \u001b[31m142.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.8/93.3 MB\u001b[0m \u001b[31m153.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/93.3 MB\u001b[0m \u001b[31m127.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/93.3 MB\u001b[0m \u001b[31m126.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/93.3 MB\u001b[0m \u001b[31m143.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/93.3 MB\u001b[0m \u001b[31m122.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/93.3 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.2/93.3 MB\u001b[0m \u001b[31m130.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/93.3 MB\u001b[0m \u001b[31m135.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m68.0/93.3 MB\u001b[0m \u001b[31m137.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m73.0/93.3 MB\u001b[0m \u001b[31m140.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m77.7/93.3 MB\u001b[0m \u001b[31m138.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m82.5/93.3 MB\u001b[0m \u001b[31m137.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m87.5/93.3 MB\u001b[0m \u001b[31m141.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m92.7/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m150.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.3/93.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pymsalruntime, argcomplete, questionary, ms-fabric-cli\r\n",
      "Successfully installed argcomplete-3.6.3 ms-fabric-cli-1.3.1 pymsalruntime-0.18.1 questionary-2.1.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a0e3268-556c-4d44-8736-b8a4cdf00d23",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:04:21.0273342Z",
       "execution_start_time": "2025-12-18T08:04:12.0911226Z",
       "normalized_state": "finished",
       "parent_msg_id": "838588b8-4c78-4918-b5df-84e8c28a63b8",
       "queued_time": "2025-12-18T07:27:08.1544085Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import sempy.fabric as fabric\n",
    "import time\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49a3eaf4-dada-45f9-9dcb-7f68ac376d08",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:04:21.3889432Z",
       "execution_start_time": "2025-12-18T08:04:21.0285912Z",
       "normalized_state": "finished",
       "parent_msg_id": "58630a51-4197-4647-bea8-10f104bc65ef",
       "queued_time": "2025-12-18T07:27:08.1558555Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "    if (capture_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7873582-a4e7-4e67-83ff-750b0c7acdce",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:04:21.800347Z",
       "execution_start_time": "2025-12-18T08:04:21.3900714Z",
       "normalized_state": "finished",
       "parent_msg_id": "2a7c1c55-44db-49c2-a1fe-5865ef07b2b4",
       "queued_time": "2025-12-18T07:27:08.1572604Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'abdb87ad-37f9-4139-b5ff-a984fc438dfe'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "trg_workspace_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d12fe1d-d26b-411d-9b87-726536df4b9c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:04:23.9224217Z",
       "execution_start_time": "2025-12-18T08:04:21.8015324Z",
       "normalized_state": "finished",
       "parent_msg_id": "804bca76-de68-48fb-a76a-650e7a8626aa",
       "queued_time": "2025-12-18T07:27:08.1782656Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current workspace: FUAM_V202512_Test3\n",
      "Current workspace ID: abdb87ad-37f9-4139-b5ff-a984fc438dfe\n"
     ]
    }
   ],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c3257437-01fc-4149-a9e6-210678eba47b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:06:25.0178106Z",
       "execution_start_time": "2025-12-18T08:04:23.9236737Z",
       "normalized_state": "finished",
       "parent_msg_id": "e9dfcf8b-e31e-4932-abfc-9539f45f2b59",
       "queued_time": "2025-12-18T07:27:08.180027Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_file_path = \"./builtin/data/table_definitions.snappy.parquet\"\n",
    "with open(src_file_path, 'rb') as file:\n",
    "                    content = file.read()\n",
    "trg_lakehouse_folder_path = notebookutils.fs.getMountPath('/default') + \"/Files/table_definitions/\" \n",
    "notebookutils.fs.mkdirs(f\"file://\" +trg_lakehouse_folder_path)\n",
    "with open(trg_lakehouse_folder_path + \"table_definitions.snappy.parquet\", \"wb\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09edbdcc-4c4f-418f-9714-48ff2018c880",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:07:03.4247255Z",
       "execution_start_time": "2025-12-18T08:06:25.0188888Z",
       "normalized_state": "finished",
       "parent_msg_id": "38551aab-2c47-4fef-885f-0575644c652a",
       "queued_time": "2025-12-18T07:27:08.181841Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notebookutils.lakehouse.loadTable(\n",
    "    {\n",
    "        \"relativePath\": f\"Files/table_definitions/table_definitions.snappy.parquet\",\n",
    "        \"pathType\": \"File\",\n",
    "        \"mode\": \"Overwrite\",\n",
    "        \"recursive\": False,\n",
    "        \"formatOptions\": {\n",
    "            \"format\": \"Parquet\"\n",
    "        }\n",
    "    }, \"FUAM_Table_Definitions\", \"FUAM_Config_Lakehouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad141615-240e-436f-a47e-599c09c1bd58",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "In case the last step fails, please try to run it again or go to the Init_FUAM_Lakehouse_Tables Notebook and run it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d0d2c3c-6d5a-43e6-b950-bf70dd1de35d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": "2025-12-18T08:07:32.9838514Z",
       "execution_start_time": "2025-12-18T08:07:03.4259641Z",
       "normalized_state": "finished",
       "parent_msg_id": "aeb1dd83-c07e-4dac-8d03-3e774b2d5b35",
       "queued_time": "2025-12-18T07:27:08.1834501Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUAM_Lakehouse SQL Endpoint ID: c63cf995-2271-460c-9563-8b25a70dbb10\n",
      "FUAM_Config_Lakehouse SQL Endpoint ID: 5e03e1ae-cb62-41d9-ad93-807c63a09240\n"
     ]
    }
   ],
   "source": [
    "# Refresh SQL Endpoint for Config_Lakehouse\n",
    "items = run_fab_command(f'api -X get -A fabric /workspaces/{trg_workspace_id}/items' , capture_output = True)\n",
    "for it in json.loads(items)['text']['value']:\n",
    "    if (it['displayName'] == 'FUAM_Config_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n",
    "        config_sql_endpoint = it['id']\n",
    "    if (it['displayName'] == 'FUAM_Lakehouse' ) & (it['type'] =='SQLEndpoint' ):\n",
    "        lh_sql_endpoint = it['id']\n",
    "print(f\"FUAM_Lakehouse SQL Endpoint ID: {lh_sql_endpoint}\")\n",
    "print(f\"FUAM_Config_Lakehouse SQL Endpoint ID: {config_sql_endpoint}\")\n",
    "\n",
    "try:\n",
    "    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{config_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n",
    "except:\n",
    "    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b8316de-e9e4-4d7a-a59d-d7c71cd2424e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": "2025-12-18T08:07:32.9850795Z",
       "normalized_state": "running",
       "parent_msg_id": "a1c6f3c2-8078-4039-8991-c29d705102b9",
       "queued_time": "2025-12-18T07:27:08.1848758Z",
       "session_id": "c0581530-17a8-4a74-b0a6-951fb4fc2ace",
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running job (sync) for 'Init_FUAM_Lakehouse_Tables.Notebook'...\n",
      "∟ Job instance '9c5bef98-7586-414d-8532-72f475981799' created\n",
      "∟ Timeout: no timeout specified\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: NotStarted\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: InProgress\n",
      "∟ Job instance status: Completed\n"
     ]
    }
   ],
   "source": [
    "# Fill default tables\n",
    "time.sleep(10)\n",
    "run_fab_command('job run ' + trg_workspace_name + '.Workspace/Init_FUAM_Lakehouse_Tables.Notebook -i {\"parameters\": {\"_inlineInstallationEnabled\": {\"type\": \"Bool\", \"value\": \"True\"} } }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9caead30-e459-4633-90c1-436737e395c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.statement-meta+json": {
       "execution_finish_time": null,
       "execution_start_time": null,
       "normalized_state": "waiting",
       "parent_msg_id": "e8b692d4-fa33-471f-8849-f9dd7b883114",
       "queued_time": "2025-12-18T07:27:08.1862832Z",
       "session_id": null,
       "session_start_time": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refresh FUAM_Lakehouse_SQL_Endpoint\n",
      "{\n",
      "  \"status_code\": 202,\n",
      "  \"text\": \"(Empty)\"\n",
      "}\n",
      "{\n",
      "  \"status_code\": 202,\n",
      "  \"text\": \"(Empty)\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Refresh of SQL Endpoint to make sure all tables are available\n",
    "try:\n",
    "    run_fab_command(f'api -A fabric -X post workspaces/{trg_workspace_id}/sqlEndpoints/{lh_sql_endpoint}/refreshMetadata?preview=True -i {{}} ', capture_output=True)\n",
    "    print(\"Refresh FUAM_Lakehouse_SQL_Endpoint\")\n",
    "except:\n",
    "    print(\"SQL Endpoint Refresh API failed, it is still in Preview, so there can be changes\")\n",
    "# Refresh Semantic Models on top of lakehouse\n",
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n",
    "\n",
    "for sm in semantic_model_connect_to_lakehouse:\n",
    "    sm_id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{sm} -q id\" , capture_output = True, silently_continue= True)\n",
    "    run_fab_command(f'api -A powerbi -X post datasets/{sm_id}/refreshes -i  {{ \"retryCount\":\"3\" }} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e15a725-70f2-4fc6-af8b-109ea6f4ce1f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
